from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
import os
from dotenv import load_dotenv
from elasticsearch import Elasticsearch
import asyncio
import json
from typing import List, Dict, Any, Optional

# Load environment variables from .env file
load_dotenv()

# Initialize Elasticsearch client
es_host = os.getenv("ES_HOST", "localhost")
es_port = os.getenv("ES_PORT", "9200")
es_index = os.getenv("ES_INDEX", "wikipedia")

es_client = Elasticsearch([f"http://{es_host}:{es_port}"])

# Define search function that returns structured data for better analysis
async def search_knowledge_base(query: str, max_results: int = 5) -> str:
    """
    Search the Elasticsearch knowledge base for information.
    
    Args:
        query: The search query
        max_results: Maximum number of results to return
        
    Returns:
        JSON string containing search results
    """
    try:
        response = es_client.search(
            index=es_index,
            body={
                "query": {
                    "multi_match": {
                        "query": query,
                        "fields": ["text", "title^2"],  # Title gets higher weight
                        "type": "best_fields"
                    }
                },
                "size": max_results
            }
        )
        
        results = []
        hits = response.get("hits", {}).get("hits", [])
        
        if not hits:
            return json.dumps({"success": False, "query": query, "message": f"No results found for query: '{query}'"})
            
        for i, hit in enumerate(hits):
            source = hit["_source"]
            title = source.get("title", "No title")
            text = source.get("text", "No content")
            score = hit["_score"]
            
            results.append({
                "rank": i+1,
                "score": score,
                "title": title,
                "content": text[:1000] + ("..." if len(text) > 1000 else "")
            })
            
        return json.dumps({
            "success": True,
            "query": query,
            "total_hits": len(hits),
            "results": results
        })
    except Exception as e:
        return json.dumps({"success": False, "query": query, "message": f"Error searching knowledge base: {str(e)}"})

# Function to generate multiple search queries
async def generate_search_queries(question: str, previous_queries: List[str] = None, 
                                 search_results: List[Dict] = None, num_queries: int = 3) -> str:
    """
    Generate multiple search queries based on a question and optionally previous search results.
    
    Args:
        question: The original question to answer
        previous_queries: List of previously tried queries (optional)
        search_results: List of previous search results (optional)
        num_queries: Number of queries to generate
        
    Returns:
        JSON string containing new search queries
    """
    prompt = f"""
    Based on the original question: "{question}"
    
    Generate {num_queries} different search queries that could help find information to answer this question.
    """
    
    if previous_queries:
        prompt += f"""
        
        These queries have been tried already:
        {json.dumps(previous_queries, indent=2)}
        """
    
    if search_results:
        prompt += f"""
        
        Here are the previous search results:
        {json.dumps(search_results, indent=2)}
        
        Based on these results, generate new queries that might find more relevant information.
        """
        
    prompt += """
    
    Return your response as a JSON array of strings, with each string being a search query.
    """
    
    # In a real implementation, you'd call your language model here
    # For simplicity, we'll return example queries
    if not previous_queries:
        # First set of queries if no previous queries
        example_queries = [
            f"{question}",
            f"wikipedia {question}",
            f"facts about {question}"
        ]
    else:
        # Refined queries (this would normally be generated by the language model)
        example_queries = [
            f"definition of {question}",
            f"details {question} when year date",
            f"explanation {question} history context"
        ]
    
    return json.dumps(example_queries)

# Function to analyze search results and determine if the answer was found
async def analyze_search_results(question: str, search_results: List[Dict], max_tokens: int = 1500) -> str:
    """
    Analyze search results to determine if they answer the original question.
    
    Args:
        question: The original question
        search_results: List of search results to analyze
        max_tokens: Maximum tokens to include in prompt
        
    Returns:
        JSON string with analysis results including if answer was found and the answer itself
    """
    prompt = f"""
    Original question: "{question}"
    
    Based on the following search results, determine if the question can be answered.
    If it can be answered, provide the answer with citations to specific search results.
    If it cannot be answered completely, identify what information is missing.
    
    Search results:
    {json.dumps(search_results, indent=2)}
    
    Return your response as a JSON object with these fields:
    - answer_found (boolean): whether the question can be answered with these results
    - answer (string): the answer to the question if found, otherwise null
    - missing_information (string): description of what information is missing, if answer_found is false
    - confidence (number): confidence score between 0-1
    - supporting_evidence (array): list of evidence from the search results that support the answer
    """
    
    # In a real implementation, you'd call your language model here
    # For demonstration, we'll return a placeholder
    example_analysis = {
        "answer_found": False,
        "answer": None,
        "missing_information": "This would be determined by the language model based on search results",
        "confidence": 0.0,
        "supporting_evidence": []
    }
    
    return json.dumps(example_analysis)

# Define a model client
model_client = OpenAIChatCompletionClient(
    model="gpt-4",  # Using a more capable model for complex reasoning
    api_key=os.getenv("OPENAI_API_KEY"),
)

# Define the advanced knowledge agent
advanced_knowledge_agent = AssistantAgent(
    name="advanced_knowledge_agent",
    model_client=model_client,
    tools=[search_knowledge_base, generate_search_queries, analyze_search_results],
    system_message="""
You are an advanced research assistant that answers questions using a Wikipedia knowledge base through Elasticsearch.
Follow this systematic approach for every question:

1. GENERATE INITIAL QUERIES:
   - Start by generating 3-5 different search queries for the user's question
   - Queries should be diverse to cover different aspects and phrasings

2. EXECUTE SEARCHES & ANALYZE RESULTS:
   - Search the knowledge base with each query
   - Analyze search results to determine if they contain the answer
   - Extract relevant information from search results

3. ITERATIVE REFINEMENT:
   - If the answer isn't found, generate new queries based on what you've learned
   - Consider synonyms, different phrasings, or more specific terms
   - Use information from previous searches to guide your new queries

4. TERMINATION:
   - Continue until you find a complete answer OR
   - Reach a maximum of 5 search iterations OR
   - Determine that the knowledge base likely doesn't contain the answer

5. ANSWER FORMULATION:
   - When answering, cite specific sources from the search results
   - Clearly distinguish between facts from the knowledge base and your reasoning
   - Structure your final answer with the most relevant information first

Make your thinking process explicit - explain what queries you're trying and why.
Remember that each tool call counts as one API call, so be strategic about your searches.
""",
    reflect_on_tool_use=True,
    model_client_stream=True,  # Enable streaming tokens from the model client
)

# Now let's create a runner function that orchestrates the whole process
async def answer_from_knowledge_base(question: str, max_iterations: int = 5) -> Dict[str, Any]:
    """
    Answer a question using the knowledge base with iterative search.
    
    Args:
        question: The question to answer
        max_iterations: Maximum number of search iterations
        
    Returns:
        Dictionary with the final answer and search history
    """
    all_queries = []
    all_results = []
    iterations = 0
    answer_found = False
    final_answer = None
    
    while iterations < max_iterations and not answer_found:
        iterations += 1
        
        # Generate search queries
        if iterations == 1:
            # Initial queries
            queries_json = await generate_search_queries(question)
        else:
            # Refined queries based on previous results
            queries_json = await generate_search_queries(question, all_queries, all_results)
            
        queries = json.loads(queries_json)
        all_queries.extend(queries)
        
        # Execute searches
        iteration_results = []
        for query in queries:
            results_json = await search_knowledge_base(query)
            results = json.loads(results_json)
            if results.get("success", False):
                iteration_results.extend(results.get("results", []))
                all_results.append({
                    "query": query,
                    "results": results.get("results", [])
                })
        
        # Analyze results to see if we found an answer
        if iteration_results:
            analysis_json = await analyze_search_results(question, iteration_results)
            analysis = json.loads(analysis_json)
            
            if analysis.get("answer_found", False):
                answer_found = True
                final_answer = analysis
                break
    
    return {
        "question": question,
        "answer_found": answer_found,
        "final_answer": final_answer,
        "iterations": iterations,
        "search_history": all_results
    }

# Run the agent and stream the messages to the console
async def main() -> None:
    # You can replace the task with any question you want to ask
    task = "Who was the first person to walk on the moon and when did it happen?"
    await Console(advanced_knowledge_agent.run_stream(task=task))
    # Close the connection to the model client
    await model_client.close()

if __name__ == "__main__":
    asyncio.run(main())